{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering sentence data for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'francais' # edit this per word \n",
    "permutations = [' francais ',' Francais ',' FRANCAIS ',] # add in list of permutations to account for plural, gender and spelling variations\n",
    "language = 'fr' # add the language you want sentence data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a string of permutations for regex\n",
    "permutations_string = '|'.join(permutations)\n",
    "permutations_string = permutations_string.strip('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the file path variable according to which language you want sentence data for \n",
    "if language == 'en':\n",
    "    folder_name = 'EN_Data/'\n",
    "elif language == 'fr':\n",
    "    folder_name = 'FR_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr_meta_part_102.jsonl', 'fr_meta_part_100.jsonl', '.DS_Store', 'fr_meta_part_101.jsonl', 'fr_meta_part_103.jsonl', 'fr_meta_part_10.jsonl', 'fr_meta_part_1.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# getting the list of data files, a sub-set of OSCAR corpus \n",
    "from os import listdir\n",
    "files = listdir(path = folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "for name in files[:2]: # edit the index of the files if the word is common to reduce runtime\n",
    "    with open(folder_name+name,'r') as json_file:\n",
    "        try:\n",
    "            result = [json.loads(jline) for jline in json_file]\n",
    "        except UnicodeDecodeError:\n",
    "             continue\n",
    "    longlist = []\n",
    "    for item in result:\n",
    "            for p in permutations:\n",
    "                if p in item['content']:\n",
    "                    longlist+=[item['content']]\n",
    "    pattern = r'(?<!,)[^.]{,50}(?:'+permutations_string+r')[^.]{,50}' # regex to find variations of target word\n",
    "    punct = r'[\",#$%&\\'\\(\\)\\*\\+\\/:;<=>@\\[\\\\\\]^`{\\|}~»«]+'\n",
    "    spaces = r'[\\.!\\?\\-_]+'\n",
    "    shortertext = []\n",
    "    for line in longlist:\n",
    "        line = str(line)\n",
    "        mo = re.findall(pattern,line) # finding all matches for regex\n",
    "        if mo != None:\n",
    "            for item in mo: # making the phrases lowercase and removing punctuation\n",
    "                item = item.lower()\n",
    "                item = re.sub(punct,'',item)\n",
    "                item = re.sub(spaces,' ',item) \n",
    "                sentence_list = item.split()\n",
    "                if len(sentence_list)>50: # truncating the phrase to make it more likely to fit in the model\n",
    "                    shorter = sentence_list[:50]\n",
    "                    short = ' '.join([str(element) for element in shorter])\n",
    "                    shortertext +=[short]\n",
    "                else:\n",
    "                    shortertext+=[item]\n",
    "    no_doubles = list(dict.fromkeys(shortertext)) # removing duplicates\n",
    "    no_doubles_shorter = no_doubles[:1000]\n",
    "    with open(word+'_'+language+'_unique.txt','a') as unique:\n",
    "        for item in no_doubles_shorter:\n",
    "                unique.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097\n"
     ]
    }
   ],
   "source": [
    " # checking length and initiating variable of sentences\n",
    "with open(word+'_'+language+'_unique.txt','r') as final_list:\n",
    "    sentences = final_list.readlines()\n",
    "    print(len(sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
